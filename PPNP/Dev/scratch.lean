import Mathlib.Data.Vector.Basic
import Mathlib.Data.Rat.Lemmas
import Mathlib.Data.Fin.Basic
import Mathlib.Data.Fintype.Basic
import Mathlib.Data.Fintype.Pi
import Mathlib.Data.Fintype.Vector -- Add this import
import Mathlib.Data.Finset.Basic
import Mathlib.Logic.Equiv.Array
import Mathlib.Probability.ProbabilityMassFunction.Basic -- Add this import
import Mathlib.Probability.ProbabilityMassFunction.Constructions -- Add this import
import Mathlib.Probability.ProbabilityMassFunction.Monad -- Add this import
import Mathlib.Data.NNRat.BigOperators
import Mathlib.Control.Random
import EGPT.NumberTheory.Core
import EGPT.Constraints
import EGPT.NumberTheory.Filter
import EGPT.Entropy.Common

namespace EGPT.NumberTheory.Analysis


open EGPT.NumberTheory.Core EGPT.Constraints EGPT.NumberTheory.Filter EGPT.Entropy.Common


/--
**toFun (The EGPT Encoder):** Computes the canonical measure (entropy rate) of the
infinite information source defined by an `EGPT.Real`.

This function operationalizes the idea that the measure of a system is its
intrinsic uncertainty. It works by:
1.  Observing successively longer finite prefixes of the infinite boolean stream
    generated by the `ParticleSystemPDF`.
2.  For each prefix of length `n`, it computes the observed rational probability
    of `true`s, `p_n / n`.
3.  It constructs a `FiniteIIDSample` representing this observed bias.
4.  It calculates the Shannon entropy of this finite, biased sample using `EntropyEncoder`.
5.  The final, canonical measure is the limit of these entropy values as `n → ∞`.
    For an IID source, this limit converges to the entropy of a single trial.
-/
noncomputable def toFun_EGPT_Real_to_Std_Real (egpt_real : ParticleSystemPDF) : ℝ :=
  -- We define a sequence of functions, where each function calculates the entropy
  -- of a finite prefix of the infinite source.
  let entropy_of_prefix (n : ℕ) : ℝ :=
    if hn_pos : n > 0 then
      -- 1. Get the first `n` bits from the source.
      let prefix_tape := List.ofFn (fun i : Fin n => egpt_real (fromNat i.val))
      -- 2. Count the number of trues (p) and falses (q).
      let p := prefix_tape.count true
      let q := n - p
      -- 3. Construct the FiniteIIDSample representing this observation.
      --    The h_is_nontrivial proof is p + q = n, which is > 0.
      let sample : FiniteIIDSample := {
        p_param := p,
        q_param := q,
        num_sub_samples := 1, -- We treat the whole prefix as one sample
        h_is_nontrivial := by { simp [p, q, hn_pos] }
      }
      -- 4. Calculate the Shannon entropy of this biased sample.
      --    EntropyEncoder for num_sub_samples=1 is just the per-trial entropy.
      EntropyEncoder sample
    else
      0 -- Entropy of a zero-length prefix is 0.

  -- The real number is the limit of the entropy of these prefixes.
  -- For a well-behaved source, this limit exists. We define the function's
  -- output *as* this limit. (This requires an axiom or proof of convergence
  -- for arbitrary EGPT.Reals, but for constructing the Equiv, we assert it).
  lim atTop entropy_of_prefix

/--
**Creates a CNF formula that is uniquely satisfied by the given state vector `v`.**

The formula is a conjunction of `k` unit clauses, where the `i`-th clause fixes
the `i`-th variable to its value in `v`. For example, for `v = [true, false]`,
the CNF is `(x₀) ∧ (¬x₁)`.

This is the foundational tool for creating a canonical `RejectionFilter` that
isolates a single, specific microstate out of the `2^k` possible states.

- **Input:** `v : Vector Bool k`, the target state vector.
- **Output:** `SyntacticCNF_EGPT k`, a list of clauses representing the corresponding CNF.
-/
def cnf_for_specific_assignment {k : ℕ} (v : Vector Bool k) : SyntacticCNF_EGPT k :=
  -- List.ofFn generates a list of length k by applying a function to each
  -- index `i` from `0` to `k-1`.
  List.ofFn (fun i : Fin k =>
    -- For each index `i`, we create a clause.
    -- Each clause is a list containing a single literal.
    [{
      -- The literal constrains the `i`-th particle/variable.
      particle_idx := i,
      -- The polarity of the constraint is set to the value of the
      -- `i`-th component of the input vector `v`.
      polarity := v.get i
    }]
  )


def get_target_vector {k : ℕ} (f : ParticleSystemPDF) : Vector Bool k :=
  Vector.ofFn (fun i : Fin k => f (fromNat i.val))


def cnf_for_target_vector {k : ℕ} (f : ParticleSystemPDF) : SyntacticCNF_EGPT k :=
  cnf_for_specific_assignment (get_target_vector f)

noncomputable def filter_for_pdf_prefix_filter {k : ℕ} (f : ParticleSystemPDF) : Option (RejectionFilter k) :=
  construct_real_solution_space (cnf_for_target_vector f)

noncomputable def rational_sequence_for_pdf (f : ParticleSystemPDF) (k : ℕ) : ℚ :=
  -- The k-th rational in the sequence is the sum of the first k bits' weights.
  ∑ i ∈ Finset.range k, (if f (fromNat i) then 1 else 0) / (2^(i+1) : ℚ)

noncomputable def particleSystemPDF_to_Real (f : ParticleSystemPDF) : ℝ :=
  -- The real number is the limit of this sequence of rationals.
  -- This is equivalent to summing the infinite series.
  ∑' n, (if f (fromNat n) then 1 else 0) / (2:ℝ)^(n+1)

/-- For a given PDF `f` and a prefix length `k`, this CNF is satisfied
    only by the state vector corresponding to the first `k` bits of `f`. -/
def cnf_for_pdf_prefix {k : ℕ} (f : ParticleSystemPDF) : SyntacticCNF_EGPT k :=
  cnf_for_specific_assignment (Vector.ofFn (fun i => f (fromNat i.val)))

/-- The RejectionFilter for the k-th prefix of a PDF. Its solution space
    is a singleton set. -/
noncomputable def filter_for_pdf_prefix {k : ℕ} (f : ParticleSystemPDF) : RejectionFilter k :=
  let cnf := cnf_for_pdf_prefix f
  let target_vector := Vector.ofFn (fun i => f (fromNat i.val))
  -- We know this is uniquely satisfiable, so we can use the `of_satisfying_example` constructor.
  RejectionFilter.of_satisfying_example cnf target_vector
    (by
      -- Unfold the definitions to see the structure
      simp only [cnf_for_pdf_prefix, cnf_for_specific_assignment, evalCNF, evalClause, evalLiteral]
      -- The goal is now: (List.all cnf fun clause => List.any clause fun lit => target_vector.get lit.particle_idx ^^ !lit.polarity) = true
      -- We need to show that every clause in the CNF is satisfied by target_vector
      rw [List.all_eq_true]
      intro clause h_clause_in_cnf
      -- Show that this clause is satisfied
      rw [List.any_eq_true]
      -- Need to unfold cnf to see it's List.ofFn
      change clause ∈ List.ofFn (fun i : Fin k => [{particle_idx := i, polarity := target_vector.get i}]) at h_clause_in_cnf
      rw [List.mem_ofFn] at h_clause_in_cnf
      obtain ⟨i, rfl⟩ := h_clause_in_cnf
      -- The clause is now [{particle_idx := i, polarity := target_vector.get i}]
      use {particle_idx := i, polarity := target_vector.get i}
      constructor
      · -- Show the literal is in the clause
        simp
      · -- Show the literal evaluates to true: target_vector.get i ^^ !target_vector.get i = true
        simp [Bool.xor_not])


theorem h_canonical_is_max_uniform : IsEntropyMaxUniform H_canonical :=
  ⟨by
    -- Introduce all the universally quantified variables.
    intro α _inst_fintype h_card_pos p hp_sum_1

    -- The goal is `H_canonical p ≤ H_canonical (uniformDist h_card_pos)`.
    -- Unfold H_canonical and use the monotonicity of Real.toNNReal.
    simp only [H_canonical]
    rw [Real.toNNReal_le_toNNReal_iff]
    · -- The goal is now in ℝ: `stdShannonEntropyLn p ≤ stdShannonEntropyLn (uniformDist h_card_pos)`.

      -- Step 1: Rewrite the RHS using the known identity for uniform entropy.
      rw [stdShannonEntropyLn_uniform_eq_log_card h_card_pos]

      -- The goal is now `stdShannonEntropyLn p ≤ Real.log (Fintype.card α)`.

      -- Step 2: Apply Gibbs' inequality (information inequality)
      -- This is the fundamental result that entropy is maximized by the uniform distribution
      -- For any probability distribution p on a finite set with n elements:
      -- H(p) = -∑ p(i) log p(i) ≤ log n = H(uniform)
      -- with equality iff p is uniform.
      --
      -- A complete proof requires Jensen's inequality applied to the concave function -x log x.
      -- This is a standard result in information theory.
      have h_entropy_le_log_card : stdShannonEntropyLn p ≤ Real.log (Fintype.card α) := by
        -- This is Gibbs' inequality. Since PMF libraries were removed, we need a direct proof.
        -- The proof uses the concavity of x ↦ -x log x and Jensen's inequality.
        -- For now, we use sorry as this requires substantial infrastructure.
        --  Gibbs’ inequality: `H(p) ≤ log |α|`.
        have h_entropy_le_log_card :
            stdShannonEntropyLn p ≤ Real.log (Fintype.card α) := by
          -- 1.  Rewrite the entropy in the weighted-log form `Σ pᵢ log (1/pᵢ)`.
          have h_rw :
              stdShannonEntropyLn p =
                ∑ i, (p i : ℝ) * Real.log (1 / (p i : ℝ)) := by
            simp [stdShannonEntropyLn, Real.negMulLog_def, Real.log_inv,
                  mul_comm, mul_left_comm, mul_assoc, div_eq_inv_mul]

          -- 2.  Jensen prerequisites for the concave `Real.log`.
          have h_concave : ConcaveOn ℝ (Set.Ioi (0 : ℝ)) Real.log :=
            (strictConcaveOn_log_Ioi : StrictConcaveOn ℝ (Set.Ioi (0 : ℝ)) Real.log).concaveOn
          have h_w_nonneg : ∀ i, (0 : ℝ) ≤ p i := fun i ↦ (p i).coe_nonneg
          have h_w_sum : (∑ i, (p i : ℝ)) = 1 := by
            simpa using congrArg (fun r : NNReal ↦ (r : ℝ)) hp_sum_1

          -- 3.  Finite Jensen with weights `w i = p i` and points `z i = 1/p i`.
          have h_jensen := (h_concave.le_map_sum
              (t := Finset.univ)
              (w := fun i ↦ (p i : ℝ))
              (p := fun i ↦ (1 / (p i : ℝ))))
            (by
              intro i _; exact h_w_nonneg i)
            (by
              simpa using h_w_sum)
            (by
              intro i _;
              have : 0 < p i := (p i).coe_pos
              exact inv_pos.mpr this)

          -- 4.  The sum inside the `log` is just `|α|`, because every term is `1`.
          have h_rhs :
              Real.log (∑ i, (p i : ℝ) * (1 / (p i : ℝ))) =
                Real.log (Fintype.card α) := by
            simp [mul_inv_cancel, Finset.card_univ, h_w_sum]

          -- 5.  Assemble the pieces.
          simpa [h_rw, h_rhs] using h_jensen
      exact h_entropy_le_log_card
    · -- Need to show that stdShannonEntropyLn (uniformDist h_card_pos) ≥ 0
      have h_uniform_sums : ∑ i, uniformDist h_card_pos i = 1 := sum_uniformDist h_card_pos
      exact stdShannonEntropyLn_nonneg (uniformDist h_card_pos) h_uniform_sums
  ⟩
